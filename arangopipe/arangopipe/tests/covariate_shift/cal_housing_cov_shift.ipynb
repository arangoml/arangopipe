{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Covariate Shift in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models predict a response based on predictor variables. The model parameters are etimated from the data. When the underlying data distribution associated with the predictors changes, the model and its associated parameters that were determined based on a previous batch of data, may not be optimal for the data that we are now seeing. This is known as **covariate shift**. Ok, so all that is well and good, but how do we detect that the underlying dataset distribution has changed? We can employ a simple technique to do that and we will illustrate that in this notebook.\n",
    "\n",
    "The problem context is as follows. We have two batches of data. One is the version used to build the current model in production and the other is the batch of data that we have received since the model has been deployed. The question is **is the current batch of data different in a distributional sense from the one used to build the current model?** We will use machine learning to solve this problem. We will tag the data from the batch used to build the current production model as $0$ and the batch of data that we have received since then as $1$. We will develop a model to discriminate these two labels. If the model we develop can discriminate very well between data from these two batches, then **covariate shift** has occured and we need to revisit modeling. If the model cannot discriminate well between these two batches, for example, the classifier we develop produces an accuracy of about $0.5$ then this classifier is not very discriminatory. It only performs as well as tossing a fair coin. If we observe such a result, then we conclude that sufficient dataset shift has not occured and our current model will serve us well.\n",
    "\n",
    "We illustrate this idea with the data from the **california housing** dataset (available in the UCI machine learning repository). The machine learning task associated with the dataset is to predict the **median house value** given a set of predictors. The rest of the notebook illustrates the idea discussed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fp = \"cal_housing.csv\"\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_cols = df.columns.tolist()\n",
    "req_cols.remove(\"medianHouseValue\")\n",
    "df = df[req_cols]\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lat\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "When we plot the histogram of the **lat** variable, we see two populations (see below):\n",
    "1. A group with **lat** values less than -119\n",
    "2. A group with **lat** values greater than -119\n",
    "Lets pretend that the current batch of data used to develop our regression model is the first one. We have now received the second batch. Can we discriminate between the two. Lets develop a classifier and see if we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "df[\"lat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.query(\"lat <= -119\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.query(\"lat > -119\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the dataset shift api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arangopipe.arangopipe_analytics.rf_dataset_shift_detector import RF_DatasetShiftDetector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfd = RF_DatasetShiftDetector()\n",
    "score = rfd.detect_dataset_shift(df1, df2)\n",
    "print (\"Detaset shift score : \", score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the score reported by the shift detector\n",
    "The API uses a classifier to discriminate between the datasets provided to it. The score reported by the API is the accuracy of the classifier to discriminate between the datasets. Values close to $0.5$ indicate that the classifier in not able to discriminate between the two datasets. This could be interpretted as a situation where no discernable shift has occured in the data since the last model deployment. Values close $1$ indicate that dataset shift is discernable and that we may need to revisit modeling. How dataset shift affects the performance of the deployed model is problem dependent. So the score must be assessed in the context of a particular application. An experiment to track the loss of model accuracy with the observed score could provide insights into a threshold score beyond which a model redevelopment is needed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
