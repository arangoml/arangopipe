{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# Getting Started with Arangopipe\n",
    "\n",
    "## Overview\n",
    "The purpose of this notebook is to provide a brief overview of the **Arangopipe** API with examples. This should provide a template for users to adapt **Arangopipe** to their specific needs. The examples cover various components of a typical machine learning stack. Test data generation utilities are also provided. The examples discussed in this notebook are in the _examples_ directory of the **Arangopipe** docker container. Examples of integration with common components of a typical machine learning stack are provided. The _examples_ directory contains sub-directories such as, _pytorch_, _mlflow_ etc.. Each of these sub-directories provide an illustration of integrating **Arangopipe** with a particular machine learning stack component.\n",
    "\n",
    "## Arangopipe API\n",
    "The python script _arangopipe_test_cases.py_ provides examples of the **Arangopipe** API. Examples provided cover the following features:\n",
    "1. Provision a Project (`provision_project()`)\n",
    "2. Register a dataset (`register_dataset()`)\n",
    "3. Lookup a dataset (`lookup_dataset()`)\n",
    "4. Register a featureset (`register_featureset()`)\n",
    "5. Lookup a featureset (`lookup_featureset()`)\n",
    "6. Register a model (`register_model()`)\n",
    "7. Lookup a model (`lookup_model()`)\n",
    "8. Log the data for machine learning experiment execution (`log_run()`)\n",
    "9. Track a model deployed to production in **Arangopipe** (`provision_deployment()`)\n",
    "10. Log the serving performance of deployed model (`log_servingperf()`)\n",
    "\n",
    "The parameters that are needed to be specified and an example of using each API is provided. \n",
    "\n",
    "\n",
    "\n",
    "## Pytorch\n",
    "The python scripts in the pytorch directory provide an illustration of how __Arangopipe__ can be used _pytorch_. The scripts provided implement a __linear__ __regression__ model in _pytorch_. The example is illustrated using the *california housing* dataset that is available in the **UCI** machine learning repository. The file *ch_data_loader.py*,  implements the details of preprocessing the dataset and generating the __metadata__ for *featuresets* and *datasets*. The file *ch_linear_regression_model.py*, implements the __linear__ __regression__ model in _pytorch_. The file *ch_torch_linear_regression_driver.py*, provides the details of using the data loader and the model classes to implement an optimization based approach to learning a __linear__ __regression__ model. The details of persisting the __metadata__ (dataset and featureset information, model parameters, optimization parameters etc.) using __Arangopipe__ are provided. To run the model, run the *run_driver()* method in *ch_torch_linear_regression_driver.py*.\n",
    "\n",
    "\n",
    "## Hyperopt\n",
    "Hyperparameter optimization experiments are one of the most common tasks performed by **data scientists**. Capturing experimental results, observations and hypothesis are critical. The notebook, _hyperopt-integration.ipynb_, provides an example of how **Arangopipe** can be used with _hyperopt_ to store meta-data from hyper-parameter optimization experiments. The example illustrates how artifacts from hyper-parameter optimization experiments, for example, the parameter space specification, can be captured in **Arangopipe**. The example is illustrated using the _california housing_ dataset that is available in the **UCI** machine learning repository.\n",
    "\n",
    "\n",
    "## MLFlow\n",
    "\n",
    "Arangopipe can be used in conjuction with other tools and API that capture ML metadata, for example **MLFlow**. The data captured can be stored in **ArangoDB** using **Arangopipe**. The example is illustrated using the _wine quality_ dataset that is available in the **UCI** machine learning repository. The examples are located in the _mlflow_ directory. These examples illustrate the complete range of the meta-data capture API in **Arangopipe**. The scripts, *example_secenario_1.py*, *example_secenario_2.py* and *example_secenario_3.py*, need to be executed in the specified order. The script, *example_secenario_1.py*, provides an example of registering datasets and featuresets prior to logging machine learning experiment results. The script, *example_secenario_2.py*, provides an example of using the lookup api to retrieve meta-data information from **Arangopipe**. The script, *example_secenario_3.py*, provides an example of tagging a model from a machine learning experiment for deployment. A model that is tagged for deployment can be tracked in **Arangopipe** using the administrative API. Look at the implementation of the `test_provision_deployment()` method in *arangopipe_test_cases.py* for an illustration.  \n",
    "\n",
    "\n",
    "\n",
    "## Scikit-learn\n",
    "Many machine learning tasks do not need nueral network models. Consequently, the machine learning stack in such cases may not use tools like _Tensorflow_ or _Pytorch_. A library like _scikit-learn_ may be adequete. The examples in the _mlflow_ directory or the _hypeopt_ directory show how **Arangopipe** can be used with tools like _scikit-learn_.\n",
    "\n",
    "\n",
    "\n",
    "## Dataset Shift Detection\n",
    "Dataset shift refers to the change in distribution of the data over time since the last model deployment. The characteristics of the data that is being received since the last model deployment may be different from what was observed during model development. This could mean that the model deployed may not be optimal. If dataset shift is a known issue to contend with, techniques such as *online learning* may be appropriate. If this shift is slow and not well characterized, a simple option to deal with dataset shift is to simply redeploy a new model when dataset shift has occured. That brings us to the question of how dataset shift can be identified. An API is provided for this purpose in **Arangopipe**. The API is based on using a classifier to differentiate between the data that was used for model development from the data that we are now receiving. The accuracy of the classifier can be used as an indicator of dataset shift. If the classifier reports accuracies of about $0.5$, that implies that the classifier does no better than flipping a fair coin to discriminate between the data that we are receiving now and the data that was used to develop the model. An accuracy close to $1$ means that the classifier can discriminate between instances of the data used to develop the model and the data that is being received now. When this happens, we may need to revisit model devlopment. An example of using the API is provided in the folder *covariate_shift*.  \n",
    "\n",
    "\n",
    "## Test Data Generation Utilities\n",
    "A utility that simulates recieving a continuous batch of datasets is provided with **Arangopipe**. This can be used for test data generation. The python script, *generate_model_data.py* is found in the *test_data_generator* directory. This utility simulates the generation of model meta-data on a monthly basis over a two year period. The script generates data for each month and records the corresponding meta-data in **Arangopipe**. The data can be generated by invoking the  `generate_runs()` from the script.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
